{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {},
  "cells": [
    {
      "id": "9c09404b",
      "cell_type": "markdown",
      "source": "# IP2 – MEx (Multimodal Exercise) klasifikacija\n\nOvaj notebook izvršava kompletan postupak koji se traži u zadatku:\n\n- učitavanje i konstrukcija atributa (feature extraction)\n- preprocesiranje (inf→NaN, imputacija medianom, standardizacija)\n- vizuelizacija (PCA 2D i 3D)\n- klasifikacija sa **≥5 algoritama**\n- poređenje modela sa:\n  - **svim atributima** (baseline)\n  - **PCA redukcijom** (95% varijanse)\n  - **SelectKBest** (mutual information, k=50)\n- validacija bez curenja podataka (GroupKFold po `subject`)\n- čuvanje izlaza za reprodukciju (`outputs/`)\n",
      "metadata": {}
    },
    {
      "id": "222a32e9",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "import os, re, glob\nimport numpy as np\nimport pandas as pd\n\nfrom scipy.stats import skew, kurtosis\n\nfrom sklearn.model_selection import GroupKFold, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nimport matplotlib.pyplot as plt\n\npd.set_option(\"display.max_columns\", 200)\nnp.random.seed(42)\n\nDATA_DIR = \"data\"          # očekuje se da je MEx dataset lokalno ovde (vidi README u dataset-u)\nOUT_DIR  = \"outputs\"\nos.makedirs(OUT_DIR, exist_ok=True)\n",
      "outputs": []
    },
    {
      "id": "63c47045",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# -----------------------------\n# 1) Učitavanje i parsiranje fajlova\n# -----------------------------\n\n# Ovaj regex odgovara tipičnim nazivima iz MEx skupa (subject, exercise_id, exercise, sensor)\n# Ako ti se razlikuje šema imenovanja fajlova, samo prilagodi regex u FNAME_RE.\nFNAME_RE = re.compile(\n    r\"(?P<subject>subject\\d+)[^\\d]*(?P<exercise_id>\\d+)[^A-Za-z]*(?P<exercise>[A-Za-z_]+)[^A-Za-z]*(?P<sensor>ACT|ACW|PM|DC)\",\n    re.IGNORECASE\n)\n\ndef parse_file_info(path: str):\n    fname = os.path.basename(path)\n    m = FNAME_RE.search(fname)\n    if not m:\n        return None\n    return {\n        \"subject\": m.group(\"subject\").lower(),\n        \"exercise_id\": int(m.group(\"exercise_id\")),\n        \"exercise\": m.group(\"exercise\").lower(),\n        \"sensor\": m.group(\"sensor\").lower()\n    }\n\ndef read_act_acw_csv(path: str):\n    # ACT/ACW imaju 3 ose + vreme (ponekad) -> uzimamo prva 3 stuba\n    df = pd.read_csv(path, header=None)\n    if df.shape[1] < 3:\n        return None\n    M = df.iloc[:, :3].values.astype(float)\n    return M\n\ndef read_pm_dc_csv(path: str):\n    # PM/DC u dataset-u često imaju drugačiji format; ovde uzimamo sve numeričke kolone\n    df = pd.read_csv(path, header=None)\n    df = df.apply(pd.to_numeric, errors=\"coerce\")\n    df = df.dropna(axis=1, how=\"all\")\n    if df.shape[1] == 0:\n        return None\n    M = df.values.astype(float)\n    return M\n",
      "outputs": []
    },
    {
      "id": "689506bd",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# -----------------------------\n# 2) Feature extraction\n# -----------------------------\n\ndef feats_act_like(M: np.ndarray, prefix: str):\n    if M is None or M.size == 0 or M.shape[1] < 3:\n        return {}\n    x, y, z = M[:, 0], M[:, 1], M[:, 2]\n    mag = np.sqrt(x**2 + y**2 + z**2)\n\n    feats = {}\n    for name, arr in [(\"x\", x), (\"y\", y), (\"z\", z), (\"mag\", mag)]:\n        feats[f\"{prefix}_{name}_mean\"] = np.nanmean(arr)\n        feats[f\"{prefix}_{name}_std\"]  = np.nanstd(arr)\n        feats[f\"{prefix}_{name}_min\"]  = np.nanmin(arr)\n        feats[f\"{prefix}_{name}_max\"]  = np.nanmax(arr)\n        feats[f\"{prefix}_{name}_skew\"] = skew(arr, nan_policy=\"omit\")\n        feats[f\"{prefix}_{name}_kurt\"] = kurtosis(arr, nan_policy=\"omit\")\n\n        # \"energy\" = srednja kvadratna vrednost\n        feats[f\"{prefix}_{name}_energy\"] = np.nanmean(arr**2)\n    return feats\n\ndef feats_generic(M: np.ndarray, prefix: str):\n    # Za senzore sa N kolona, izvlačimo agregate po kolonama (mean/std/min/max)\n    if M is None or M.size == 0:\n        return {}\n    feats = {}\n    for j in range(M.shape[1]):\n        col = M[:, j]\n        feats[f\"{prefix}_c{j}_mean\"] = np.nanmean(col)\n        feats[f\"{prefix}_c{j}_std\"]  = np.nanstd(col)\n        feats[f\"{prefix}_c{j}_min\"]  = np.nanmin(col)\n        feats[f\"{prefix}_c{j}_max\"]  = np.nanmax(col)\n        feats[f\"{prefix}_c{j}_energy\"] = np.nanmean(col**2)\n    return feats\n\ndef extract_features_for_file(path: str, sensor_key: str):\n    info = parse_file_info(path)\n    if info is None:\n        return None\n\n    if sensor_key in [\"act\", \"acw\"]:\n        M = read_act_acw_csv(path)\n        feats = feats_act_like(M, sensor_key)\n    else:\n        M = read_pm_dc_csv(path)\n        feats = feats_generic(M, sensor_key)\n\n    row = {**{k: info[k] for k in [\"subject\", \"exercise_id\", \"exercise\"]}, **feats}\n    return row\n\ndef build_feature_table(sensor_key: str):\n    # traži sensor podfoldere ili sve csv-ove ispod DATA_DIR\n    patt = os.path.join(DATA_DIR, \"**\", \"*.csv\")\n    paths = glob.glob(patt, recursive=True)\n\n    rows = []\n    for p in paths:\n        info = parse_file_info(p)\n        if info is None:\n            continue\n        if info[\"sensor\"] != sensor_key:\n            continue\n        row = extract_features_for_file(p, sensor_key)\n        if row is not None:\n            rows.append(row)\n\n    df = pd.DataFrame(rows)\n    if df.empty:\n        raise RuntimeError(f\"Nisam našla fajlove za senzor '{sensor_key}'. Proveri DATA_DIR i FNAME_RE.\")\n    return df\n",
      "outputs": []
    },
    {
      "id": "dcf64e6a",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# -----------------------------\n# 3) Konstrukcija full skupa atributa (ALL sensors)\n# -----------------------------\n\nact_df = build_feature_table(\"act\")\nacw_df = build_feature_table(\"acw\")\npm_df  = build_feature_table(\"pm\")\ndc_df  = build_feature_table(\"dc\")\n\nkeys = [\"subject\", \"exercise_id\", \"exercise\"]\n\n# left-merge na ACT (pretpostavka: ACT postoji za svaki primer)\ndf_all = act_df.merge(acw_df, on=keys, how=\"left\", suffixes=(\"\", \"_acwdup\"))\ndf_all = df_all.merge(pm_df,  on=keys, how=\"left\", suffixes=(\"\", \"_pmdup\"))\ndf_all = df_all.merge(dc_df,  on=keys, how=\"left\", suffixes=(\"\", \"_dcdup\"))\n\n# ukloni eventualne duplikat sufikse ako se pojave\ndf_all = df_all.loc[:, ~df_all.columns.str.contains(\"dup$\")]\n\ndf_all.head(), df_all.shape\n",
      "outputs": []
    },
    {
      "id": "2bd44d00",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# Snimi \"raw features\" (pre preprocesiranja)\nraw_path = os.path.join(OUT_DIR, \"mex_features_all_raw.csv\")\ndf_all.to_csv(raw_path, index=False)\n\nprint(\"Saved:\", raw_path)\n",
      "outputs": []
    },
    {
      "id": "d0a3eb7a",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# -----------------------------\n# 4) Preprocesiranje (inf→NaN, imputacija medianom)\n# -----------------------------\ndf_all_clean = df_all.copy()\n\n# zameni inf\ndf_all_clean = df_all_clean.replace([np.inf, -np.inf], np.nan)\n\n# imputacija medianom samo na feature kolonama\nfeature_cols = [c for c in df_all_clean.columns if c not in keys]\ndf_all_clean[feature_cols] = df_all_clean[feature_cols].apply(pd.to_numeric, errors=\"coerce\")\ndf_all_clean[feature_cols] = df_all_clean[feature_cols].fillna(df_all_clean[feature_cols].median(numeric_only=True))\n\npreproc_path = os.path.join(OUT_DIR, \"mex_features_all_preprocessed.csv\")\ndf_all_clean.to_csv(preproc_path, index=False)\n\nprint(\"Saved:\", preproc_path)\ndf_all_clean.head()\n",
      "outputs": []
    },
    {
      "id": "3bf8df3a",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# -----------------------------\n# 5) Vizuelizacija (PCA 2D i 3D)\n# -----------------------------\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df_all_clean.drop(columns=[\"exercise\", \"subject\", \"exercise_id\"])\ny = df_all_clean[\"exercise\"].values\ngroups = df_all_clean[\"subject\"].values\n\n# standardizacija samo za vizuelizaciju\nX_scaled = StandardScaler().fit_transform(X)\n\nle = LabelEncoder()\ny_enc = le.fit_transform(y)\n\n# PCA 2D\npca2 = PCA(n_components=2, random_state=42)\nZ2 = pca2.fit_transform(X_scaled)\n\nplt.figure(figsize=(8,6))\nplt.scatter(Z2[:,0], Z2[:,1], c=y_enc, s=12)\nplt.title(\"PCA 2D – ALL sensors\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.tight_layout()\nplt.show()\n\n# PCA 3D\npca3 = PCA(n_components=3, random_state=42)\nZ3 = pca3.fit_transform(X_scaled)\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection=\"3d\")\nax.scatter(Z3[:,0], Z3[:,1], Z3[:,2], c=y_enc, s=10)\nax.set_title(\"PCA 3D – ALL sensors\")\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.set_zlabel(\"PC3\")\nplt.tight_layout()\nplt.show()\n",
      "outputs": []
    },
    {
      "id": "ecb69ad1",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# -----------------------------\n# 6) Evaluacija (≥5 algoritama) + redukcija atributa\n#    - baseline: svi atributi\n#    - PCA: 95% varijanse\n#    - KBest: k=50 (mutual information)\n# -----------------------------\n\nMODELS = {\n    \"LogReg\": LogisticRegression(max_iter=3000),\n    \"SVM_RBF\": SVC(kernel=\"rbf\", C=5, gamma=\"scale\"),\n    \"RandomForest\": RandomForestClassifier(n_estimators=300, random_state=42),\n    \"KNN\": KNeighborsClassifier(n_neighbors=7),\n    \"NaiveBayes\": GaussianNB(),\n    \"HistGB\": HistGradientBoostingClassifier(random_state=42)\n}\n\ndef make_pipeline(model, reduction: str):\n    steps = [\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n        (\"scaler\", StandardScaler()),\n    ]\n    if reduction == \"pca\":\n        steps.append((\"pca\", PCA(n_components=0.95, random_state=42)))\n    elif reduction == \"kbest\":\n        steps.append((\"kbest\", SelectKBest(mutual_info_classif, k=50)))\n    steps.append((\"model\", model))\n    return Pipeline(steps)\n\ndef evaluate_all_reductions(df):\n    X = df.drop(columns=[\"exercise\", \"subject\", \"exercise_id\"])\n    y = df[\"exercise\"].values\n    groups = df[\"subject\"].values\n\n    cv = GroupKFold(n_splits=5)\n\n    rows = []\n    for reduction in [\"none\", \"pca\", \"kbest\"]:\n        for name, model in MODELS.items():\n            pipe = make_pipeline(model, reduction)\n\n            scores = cross_validate(\n                pipe, X, y,\n                cv=cv,\n                groups=groups,\n                scoring={\"acc\": \"accuracy\", \"f1\": \"f1_weighted\"},\n                n_jobs=-1\n            )\n            rows.append({\n                \"reduction\": reduction,\n                \"model\": name,\n                \"acc\": float(np.mean(scores[\"test_acc\"])),\n                \"f1\": float(np.mean(scores[\"test_f1\"]))\n            })\n    return pd.DataFrame(rows).sort_values([\"reduction\", \"f1\"], ascending=[True, False]).reset_index(drop=True)\n\nres = evaluate_all_reductions(df_all_clean)\nres\n",
      "outputs": []
    },
    {
      "id": "95d0858b",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# Snimi rezultate\nres_path = os.path.join(OUT_DIR, \"model_comparison_all_reductions.csv\")\nres.to_csv(res_path, index=False)\nprint(\"Saved:\", res_path)\n",
      "outputs": []
    },
    {
      "id": "0e161bde",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# Graf: F1 po modelu za svaku redukciju\nplt.figure(figsize=(10,5))\nfor r in [\"none\",\"pca\",\"kbest\"]:\n    sub = res[res[\"reduction\"] == r].sort_values(\"model\")\n    plt.plot(sub[\"model\"], sub[\"f1\"], marker=\"o\", label=r)\n\nplt.ylabel(\"F1-score (weighted)\")\nplt.title(\"Poređenje: ALL atributi vs PCA vs SelectKBest\")\nplt.xticks(rotation=45)\nplt.legend()\nplt.tight_layout()\nplt.show()\n",
      "outputs": []
    },
    {
      "id": "6e40f745",
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "source": "# -----------------------------\n# 7) Treniraj najbolji model na celom skupu (za isporuku)\n# -----------------------------\nbest = res.sort_values(\"f1\", ascending=False).iloc[0]\nbest_reduction = best[\"reduction\"]\nbest_model_name = best[\"model\"]\n\nprint(\"Best overall:\", best_model_name, \"| reduction:\", best_reduction, \"| acc:\", best[\"acc\"], \"| f1:\", best[\"f1\"])\n\nX = df_all_clean.drop(columns=[\"exercise\", \"subject\", \"exercise_id\"])\ny = df_all_clean[\"exercise\"].values\n\nbest_model = MODELS[best_model_name]\nbest_pipe = make_pipeline(best_model, best_reduction)\nbest_pipe.fit(X, y)\n\nimport joblib\nmodel_path = os.path.join(OUT_DIR, f\"best_model_all_{best_model_name}_{best_reduction}.joblib\")\njoblib.dump(best_pipe, model_path)\nprint(\"Saved:\", model_path)\n",
      "outputs": []
    },
    {
      "id": "ae7a65b6",
      "cell_type": "markdown",
      "source": "## Šta predati (materijal za reprodukciju)\n\nU folderu `outputs/` dobijaš:\n- `mex_features_all_raw.csv` (konstruisani atributi, pre preprocesiranja)\n- `mex_features_all_preprocessed.csv` (posle preprocesiranja)\n- `model_comparison_all_reductions.csv` (poređenje ≥5 algoritama za 3 varijante atributa)\n- `best_model_all_<...>.joblib` (najbolji pipeline)\n\nZa PDF deo (tekstualni deo rada) uvrsti:\n- opis skupa podataka i cilja (`exercise`)\n- opis feature extraction (statističke karakteristike po senzoru)\n- preprocesiranje (inf→NaN, median imputacija, standardizacija)\n- validaciju (GroupKFold po `subject`)\n- rezultate + grafike (PCA 2D/3D i poređenje F1)\n\n",
      "metadata": {}
    }
  ]
}